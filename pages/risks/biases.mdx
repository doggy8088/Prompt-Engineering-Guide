# Biases

# 偏見

LLMs can produce problematic generations that can potentially be harmful and display biases that could deteriorate the performance of the model on downstream tasks. Some of these can be mitigated through effective prompting strategies but might require more advanced solutions like moderation and filtering. 

LLMs 可能會產生有問題的產生結果，這些結果可能會有害，並且可能會顯示出偏見，這可能會降低模型在下游任務中的效能。其中一些問題可以透過有效的提示策略來緩解，但可能需要更高階的解決方案，如調節和過濾。

### Distribution of Exemplars

### 樣本分佈

When performing few-shot learning, does the distribution of the exemplars affect the performance of the model or bias the model in some way? We can perform a simple test here.

當進行少樣本學習時，樣本的分佈是否會影響模型的效能或以某種方式對模型產生偏見？我們可以在這裡進行一個簡單的測試。

*Prompt:*

*提示:*

```
Q: I just got the best news ever!
A: Positive

Q: We just got a raise at work!
A: Positive

Q: I'm so proud of what I accomplished today.
A: Positive

Q: I'm having the best day ever!
A: Positive

Q: I'm really looking forward to the weekend.
A: Positive

Q: I just got the best present ever!
A: Positive

Q: I'm so happy right now.
A: Positive

Q: I'm so blessed to have such an amazing family.
A: Positive

Q: The weather outside is so gloomy.
A: Negative

Q: I just got some terrible news.
A: Negative

Q: That left a sour taste.
A:
```

*Output:*

*輸出:*

```
Negative
```

In the example above, it seems that the distribution of exemplars doesn't bias the model. This is good. Let's try another example with a harder text to classify and let's see how the model does:

在上面的例子中，似乎樣本的分佈並沒有對模型產生偏差。這很好。讓我們嘗試另一個更難分類的文字例子，看看模型的表現如何：

*Prompt:*

*提示:*

```
Q: The food here is delicious!
A: Positive 

Q: I'm so tired of this coursework.
A: Negative

Q: I can't believe I failed the exam.
A: Negative

Q: I had a great day today!
A: Positive 

Q: I hate this job.
A: Negative

Q: The service here is terrible.
A: Negative

Q: I'm so frustrated with my life.
A: Negative

Q: I never get a break.
A: Negative

Q: This meal tastes awful.
A: Negative

Q: I can't stand my boss.
A: Negative

Q: I feel something.
A:
```

*Output:*

*輸出:*

```
Negative
```

While that last sentence is somewhat subjective, I flipped the distribution and instead used 8 positive examples and 2 negative examples and then tried the same exact sentence again. Guess what the model responded? It responded "Positive". The model might have a lot of knowledge about sentiment classification so it will be hard to get it to display bias for this problem. The advice here is to avoid skewing the distribution and instead provide a more balanced number of examples for each label. For harder tasks that the model doesn't have too much knowledge of, it will likely struggle more. 

那最後一句話有點主觀，我翻轉了分佈，使用了8個正面例子和2個負面例子，然後再次嘗試了完全相同的句子。你猜模型的回應是什麼？它回答“積極”。模型可能對情感分類有很多知識，因此很難讓它顯示出這個問題的偏見。建議避免偏斜分佈，而是為每個標籤提供更平衡的例子數量。對於模型沒有太多知識的更難的任務，它可能會更加困難。

### Order of Exemplars

### 示範順序

When performing few-shot learning, does the order affect the performance of the model or bias the model in some way?

當進行少樣本學習時，順序是否會影響模型的效能或以某種方式對模型產生偏見？

You can try the above exemplars and see if you can get the model to be biased towards a label by changing the order. The advice is to randomly order exemplars. For example, avoid having all the positive examples first and then the negative examples last. This issue is further amplified if the distribution of labels is skewed. Always ensure to experiment a lot to reduce this type of bias.

你可以嘗試上述示範，看看是否可以透過更改順序使模型偏向某個標籤。建議隨機排列示範。例如，避免先放所有正面例子，然後再放負面例子。如果標籤分佈不均，這個問題會進一步放大。一定要進行大量實驗以減少這種偏差。

