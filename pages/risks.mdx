# Risks & Misuses

# 風險和誤用

import { Callout } from 'nextra-theme-docs'

We have seen already how effective well-crafted prompts can be for various tasks using techniques like few-shot learning and chain-of-thought prompting. As we think about building real-world applications on top of LLMs, it becomes crucial to think about the misuses, risks, and safety practices involved with language models.

我們已經看到了如何使用 few-shot learning 和 chain-of-thought prompting 等技術，透過精心製作的提示對各種任務進行有效的處理。當我們考慮在 LLMs 上構建現實世界的應用時，關注語言模型的誤用、風險和安全實踐變得至關重要。

This section focuses on highlighting some of the risks and misuses of LLMs via techniques like prompt injections. It also highlights harmful behaviors and how to potentially mitigate them via effective prompting techniques. Other topics of interest include generalizability, calibration, biases, social biases, and factuality to name a few.

本節重點介紹了透過提示注入等技術來突出顯示LLMs的一些風險和誤用。它還強調了有害行為以及如何透過有效的提示技術潛在地減輕它們。其他感興趣的主題包括普遍性、校準、偏見、社會偏見和事實性等。

<Callout emoji="⚠️">
  This section is under heavy development.
</Callout>

<Callout emoji="⚠️">
  本節目前仍在大力開發中。
</Callout>

